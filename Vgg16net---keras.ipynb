{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.layers import Conv2D, MaxPooling2D, ZeroPadding2D, BatchNormalization, Activation, advanced_activations, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler\n",
    "from keras import regularizers, optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "#import GlobalAvgPool2D\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from skimage import io, transform\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy import misc\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Input,Concatenate\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "#layer1 32*32*3\n",
    "def vgg16(num_classes):\n",
    "    model = Sequential()\n",
    "    #第一个 卷积层 的卷积核的数目是32 ，卷积核的大小是3*3，stride没写，默认应该是1*1\n",
    "    #对于stride=1*1,并且padding ='same',这种情况卷积后的图像shape与卷积前相同，本层后shape还是32*32\n",
    "    weight_decay=0.0005\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',input_shape=(IMAGE_SIZE,IMAGE_SIZE,3),kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    #进行一次归一化\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.3))\n",
    "    #layer2 32*32*64\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #下面两行代码是等价的，#keras Pool层有个奇怪的地方，stride,默认是(2*2),\n",
    "    #padding默认是valid，在写代码是这些参数还是最好都加上,这一步之后,输出的shape是16*16*64\n",
    "    #model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2),strides=(2,2),padding='same')  )\n",
    "    #layer3 16*16*64\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    #layer4 16*16*128\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #layer5 8*8*128\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    #layer6 8*8*256\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    #layer7 8*8*256\n",
    "    model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #layer8 4*4*256\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    #layer9 4*4*512\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    #layer10 4*4*512\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    #layer11 2*2*512\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    #layer12 2*2*512\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.4))\n",
    "    #layer13 2*2*512\n",
    "    model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.5))\n",
    "    #layer14 1*1*512\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #layer15 512\n",
    "    model.add(Dense(512,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    #layer16 512\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    # 10\n",
    "    # model.summary()\n",
    "    return model\n",
    "\n",
    "#model.save('my_model_bp.h5')\n",
    "def loadData(IMAGE_SIZE):\n",
    "    images = os.listdir(DATA_DIR)\n",
    "    d = {}\n",
    "    train_data = []\n",
    "    train_pics=[]\n",
    "    test_pics=[]\n",
    "    test_labels=[]\n",
    "    train_labels = []\n",
    "    train_data_all=[]\n",
    "    train_pics_all=[]\n",
    "    train_labels_all = []\n",
    "    for idx, dir in enumerate(os.listdir(DATA_DIR)):\n",
    "            d[idx] = dir\n",
    "            label = idx\n",
    "            subdir = os.path.join(DATA_DIR, dir)\n",
    "            for i, file in enumerate(os.listdir(subdir)):\n",
    "                pic = misc.imread(os.path.join(subdir, file))\n",
    "                pic = misc.imresize(pic, (IMAGE_SIZE,IMAGE_SIZE, 3))\n",
    "                train_data_all.append((pic, label))\n",
    "    np.random.shuffle(train_data_all)\n",
    "    validation_size = int(len(train_data_all) * 0.20)\n",
    "    test_data = train_data_all[:validation_size].copy()\n",
    "    np.random.shuffle(test_data)\n",
    "    train_data = train_data_all[validation_size:]\n",
    "    for i in range(len(train_data)):\n",
    "        train_pics.append(train_data[i][0])\n",
    "        train_labels.append(train_data[i][1])\n",
    "    for i in range(len(test_data)):\n",
    "        test_pics.append(test_data[i][0])\n",
    "        test_labels.append(test_data[i][1])\n",
    "    for i in range(len(train_data_all)):\n",
    "        train_pics_all.append(train_data_all[i][0])\n",
    "        train_labels_all.append(train_data_all[i][1])\n",
    "    return np.array(train_pics), np.array(train_labels),np.array(test_pics), np.array(test_labels),np.array(train_pics_all), np.array(train_labels_all)\n",
    "\n",
    "np.random.seed(25)\n",
    "#加载数据\n",
    "IMAGE_SIZE = 227\n",
    "DATA_DIR = r\"G:\\科研\\数据集\\opendata_VRID\\model\"\n",
    "train_data, train_labels, test_data, test_labels,train_data_all,train_labels_all = loadData(IMAGE_SIZE)\n",
    "#输出训练和测试数据量\n",
    "# print (\"Train data size: \", len(train_data))\n",
    "# print (\"Test data size: \", len(test_data))\n",
    "#将标签量进行转化\n",
    "train_labels=np_utils.to_categorical(train_labels)\n",
    "test_labels=np_utils.to_categorical(test_labels)\n",
    "train_labels_all=np_utils.to_categorical(train_labels_all)\n",
    "#设置检查点\n",
    "# filepath=r\"G:\\科研\\数据集\\opendata_VRID\\model\\weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# callbacks_list = [checkpoint]\n",
    "#设置最佳检查点\n",
    "filepath=r\"G:\\科研\\数据集\\opendata_VRID\\weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "#设置模型\n",
    "num_classes=10\n",
    "model=vgg16(num_classes)\n",
    "#model=squeezenet(227,227,3,num_classes)\n",
    "#编译模型\n",
    "epochs = 10\n",
    "learning_rate = 0.01\n",
    "#decay_rate = learning_rate / epochs\n",
    "momentum = 0.9\n",
    "sgd = SGD(lr=learning_rate, momentum=momentum,  decay=1e-6, nesterov=False)\n",
    "model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['accuracy'])\n",
    "#sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "#model.compile(loss='binary_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "#匹配模型\n",
    "#lrate = LearningRateScheduler(step_decay)\n",
    "#callbacks_list = [lrate]\n",
    "history=model.fit(train_data_all, train_labels_all,validation_split=0.2, nb_epoch=epochs,batch_size=64,callbacks=callbacks_list)\n",
    "#测试模型\n",
    "preds = np.argmax(model.predict(test_data), axis=1)\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "print (accuracy_score(test_labels, preds))\n",
    "#保存模型\n",
    "model.save('Vgg16net-model(10,64).model')\n",
    "#记录模型日志\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(len(acc))\n",
    "#画模型准确率曲线和损失率曲线\n",
    "plt.plot(epochs, acc, 'ro:', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'bo:', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'ro:', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'bo:', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('graph.png')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
